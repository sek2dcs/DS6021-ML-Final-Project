{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: install/import packages and define paths\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "except Exception:\n",
        "    # Fallback install for environments missing OpenCV\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"opencv-python-headless\", \"pandas\", \"numpy\"])  # noqa: E402\n",
        "    import cv2  # noqa: E402\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Project paths\n",
        "PROJECT_ROOT = Path(\"/Users/bellalu/DS6012/ML1_Project\")\n",
        "IMAGES_DIR = PROJECT_ROOT / \"faces_database\"\n",
        "PHI = (1 + 5 ** 0.5) / 2  # golden ratio\n",
        "\n",
        "# Haar cascade files (bundled with OpenCV)\n",
        "HAAR_DIR = Path(cv2.data.haarcascades)\n",
        "CASCADE_FACE = str(HAAR_DIR / \"haarcascade_frontalface_default.xml\")\n",
        "CASCADE_EYES = str(HAAR_DIR / \"haarcascade_eye.xml\")\n",
        "CASCADE_NOSE = str(HAAR_DIR / \"haarcascade_mcs_nose.xml\") if (HAAR_DIR / \"haarcascade_mcs_nose.xml\").exists() else None\n",
        "CASCADE_MOUTH = str(HAAR_DIR / \"haarcascade_smile.xml\")  # smile is a common proxy for mouth region\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(CASCADE_FACE)\n",
        "eyes_cascade = cv2.CascadeClassifier(CASCADE_EYES)\n",
        "nose_cascade = cv2.CascadeClassifier(CASCADE_NOSE) if CASCADE_NOSE else None\n",
        "mouth_cascade = cv2.CascadeClassifier(CASCADE_MOUTH)\n",
        "\n",
        "assert IMAGES_DIR.exists(), f\"Images directory not found: {IMAGES_DIR}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filename parsing for categorical labels and small helpers\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "AGE_MAP = {\"y\": \"young\", \"m\": \"middle\", \"o\": \"old\"}\n",
        "GENDER_MAP = {\"m\": \"male\", \"f\": \"female\"}\n",
        "EXPR_MAP = {\"h\": \"happy\"}  # extend if more expressions exist\n",
        "\n",
        "\n",
        "def parse_labels(file_name: str) -> Dict[str, str]:\n",
        "    \"\"\"Parse ID, age, gender, expression, variant from file name like '140_y_f_h_a.jpg'.\"\"\"\n",
        "    stem = Path(file_name).stem\n",
        "    parts = stem.split(\"_\")\n",
        "    # Expected: [id, age, gender, expr, variant]\n",
        "    out = {\n",
        "        \"subject_id\": parts[0] if len(parts) > 0 else \"\",\n",
        "        \"age_group\": AGE_MAP.get(parts[1], parts[1]) if len(parts) > 1 else \"\",\n",
        "        \"gender\": GENDER_MAP.get(parts[2], parts[2]) if len(parts) > 2 else \"\",\n",
        "        \"expression\": EXPR_MAP.get(parts[3], parts[3]) if len(parts) > 3 else \"\",\n",
        "        \"variant\": parts[4] if len(parts) > 4 else \"\",\n",
        "    }\n",
        "    return out\n",
        "\n",
        "\n",
        "def euclidean_distance(p1: Tuple[int, int], p2: Tuple[int, int]) -> float:\n",
        "    return float(np.hypot(p1[0] - p2[0], p1[1] - p2[1]))\n",
        "\n",
        "\n",
        "def golden_deviation(a: float, b: float) -> Optional[float]:\n",
        "    if a is None or b is None or a <= 0 or b <= 0:\n",
        "        return None\n",
        "    r = max(a, b) / min(a, b)\n",
        "    return abs(r - PHI)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detection utilities using OpenCV Haar cascades\n",
        "\n",
        "def detect_face_regions(img_bgr):\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(80, 80))\n",
        "    if len(faces) == 0:\n",
        "        return None, None\n",
        "    # choose the largest face\n",
        "    x, y, w, h = sorted(faces, key=lambda r: r[2] * r[3], reverse=True)[0]\n",
        "    face_roi = img_bgr[y:y+h, x:x+w]\n",
        "    face_gray = gray[y:y+h, x:x+w]\n",
        "    return (x, y, w, h), (face_roi, face_gray)\n",
        "\n",
        "\n",
        "def detect_eyes(face_gray):\n",
        "    eyes = eyes_cascade.detectMultiScale(face_gray, scaleFactor=1.1, minNeighbors=8, minSize=(20, 20))\n",
        "    if len(eyes) < 2:\n",
        "        return None\n",
        "    # pick two eyes with left/right order based on x\n",
        "    eyes_sorted = sorted(eyes, key=lambda r: r[0])[:2]\n",
        "    centers = []\n",
        "    for (ex, ey, ew, eh) in eyes_sorted:\n",
        "        centers.append((int(ex + ew / 2), int(ey + eh / 2)))\n",
        "    # ensure left-right order\n",
        "    centers = sorted(centers, key=lambda c: c[0])\n",
        "    return centers  # [(x_left, y_left), (x_right, y_right)]\n",
        "\n",
        "\n",
        "def detect_nose(face_gray):\n",
        "    if not nose_cascade:\n",
        "        return None\n",
        "    noses = nose_cascade.detectMultiScale(face_gray, scaleFactor=1.1, minNeighbors=5, minSize=(24, 24))\n",
        "    if len(noses) == 0:\n",
        "        return None\n",
        "    x, y, w, h = sorted(noses, key=lambda r: r[2] * r[3], reverse=True)[0]\n",
        "    center = (int(x + w / 2), int(y + h / 2))\n",
        "    return center\n",
        "\n",
        "\n",
        "def detect_mouth(face_gray):\n",
        "    # Using smile cascade as a proxy for mouth region; not perfect but works reasonably for smiles\n",
        "    mouths = mouth_cascade.detectMultiScale(face_gray, scaleFactor=1.1, minNeighbors=40, minSize=(30, 15))\n",
        "    if len(mouths) == 0:\n",
        "        return None, None\n",
        "    x, y, w, h = sorted(mouths, key=lambda r: r[2] * r[3], reverse=True)[0]\n",
        "    center = (int(x + w / 2), int(y + h / 2))\n",
        "    width = float(w)\n",
        "    return center, width\n",
        "\n",
        "\n",
        "def compute_measurements(img_bgr) -> dict:\n",
        "    results = {\n",
        "        \"face_width\": None,\n",
        "        \"face_height\": None,\n",
        "        \"eye_distance\": None,\n",
        "        \"mouth_width\": None,\n",
        "        \"nose_to_mouth\": None,\n",
        "        # golden ratio deviations (lower is closer to phi)\n",
        "        \"phi_dev_face_h_w\": None,          # face height vs width\n",
        "        \"phi_dev_eye_to_mouth\": None,      # eye distance vs mouth width\n",
        "        \"phi_dev_nose_mouth_to_eye\": None  # nose-to-mouth vs eye distance\n",
        "    }\n",
        "\n",
        "    face_rect, face_pair = detect_face_regions(img_bgr)\n",
        "    if face_rect is None:\n",
        "        return results\n",
        "\n",
        "    (fx, fy, fw, fh) = face_rect\n",
        "    face_roi, face_gray = face_pair\n",
        "\n",
        "    results[\"face_width\"] = float(fw)\n",
        "    results[\"face_height\"] = float(fh)\n",
        "\n",
        "    # features within face\n",
        "    eye_centers = detect_eyes(face_gray)\n",
        "    nose_center = detect_nose(face_gray)\n",
        "    mouth_center, mouth_width = detect_mouth(face_gray)\n",
        "\n",
        "    if eye_centers and len(eye_centers) == 2:\n",
        "        results[\"eye_distance\"] = euclidean_distance(eye_centers[0], eye_centers[1])\n",
        "    if mouth_width is not None:\n",
        "        results[\"mouth_width\"] = float(mouth_width)\n",
        "    if nose_center is not None and mouth_center is not None:\n",
        "        results[\"nose_to_mouth\"] = euclidean_distance(nose_center, mouth_center)\n",
        "\n",
        "    # golden ratio deviations\n",
        "    results[\"phi_dev_face_h_w\"] = golden_deviation(results[\"face_height\"], results[\"face_width\"]) if results[\"face_height\"] and results[\"face_width\"] else None\n",
        "    if results[\"eye_distance\"] and results[\"mouth_width\"]:\n",
        "        results[\"phi_dev_eye_to_mouth\"] = golden_deviation(results[\"eye_distance\"], results[\"mouth_width\"])\n",
        "    if results[\"nose_to_mouth\"] and results[\"eye_distance\"]:\n",
        "        results[\"phi_dev_nose_mouth_to_eye\"] = golden_deviation(results[\"nose_to_mouth\"], results[\"eye_distance\"])\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 12 images from: /Users/bellalu/DS6012/ML1_Project/faces_database\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>age_group</th>\n",
              "      <th>gender</th>\n",
              "      <th>expression</th>\n",
              "      <th>variant</th>\n",
              "      <th>face_width</th>\n",
              "      <th>face_height</th>\n",
              "      <th>eye_distance</th>\n",
              "      <th>mouth_width</th>\n",
              "      <th>nose_to_mouth</th>\n",
              "      <th>phi_dev_face_h_w</th>\n",
              "      <th>phi_dev_eye_to_mouth</th>\n",
              "      <th>phi_dev_nose_mouth_to_eye</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>004_o_m_h_a.jpg</td>\n",
              "      <td>004</td>\n",
              "      <td>old</td>\n",
              "      <td>male</td>\n",
              "      <td>happy</td>\n",
              "      <td>a</td>\n",
              "      <td>1859.0</td>\n",
              "      <td>1859.0</td>\n",
              "      <td>682.354746</td>\n",
              "      <td>767.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.618034</td>\n",
              "      <td>0.493985</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>004_o_m_h_b.jpg</td>\n",
              "      <td>004</td>\n",
              "      <td>old</td>\n",
              "      <td>male</td>\n",
              "      <td>happy</td>\n",
              "      <td>b</td>\n",
              "      <td>1908.0</td>\n",
              "      <td>1908.0</td>\n",
              "      <td>435.491676</td>\n",
              "      <td>558.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.618034</td>\n",
              "      <td>0.336724</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>066_y_m_h_a.jpg</td>\n",
              "      <td>066</td>\n",
              "      <td>young</td>\n",
              "      <td>male</td>\n",
              "      <td>happy</td>\n",
              "      <td>a</td>\n",
              "      <td>1842.0</td>\n",
              "      <td>1842.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>709.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.618034</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>066_y_m_h_b.jpg</td>\n",
              "      <td>066</td>\n",
              "      <td>young</td>\n",
              "      <td>male</td>\n",
              "      <td>happy</td>\n",
              "      <td>b</td>\n",
              "      <td>1824.0</td>\n",
              "      <td>1824.0</td>\n",
              "      <td>1025.914226</td>\n",
              "      <td>725.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.618034</td>\n",
              "      <td>0.202980</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>079_o_f_h_a.jpg</td>\n",
              "      <td>079</td>\n",
              "      <td>old</td>\n",
              "      <td>female</td>\n",
              "      <td>happy</td>\n",
              "      <td>a</td>\n",
              "      <td>2193.0</td>\n",
              "      <td>2193.0</td>\n",
              "      <td>239.384628</td>\n",
              "      <td>509.0</td>\n",
              "      <td>None</td>\n",
              "      <td>0.618034</td>\n",
              "      <td>0.508251</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         file_name subject_id age_group  gender expression variant  \\\n",
              "0  004_o_m_h_a.jpg        004       old    male      happy       a   \n",
              "1  004_o_m_h_b.jpg        004       old    male      happy       b   \n",
              "2  066_y_m_h_a.jpg        066     young    male      happy       a   \n",
              "3  066_y_m_h_b.jpg        066     young    male      happy       b   \n",
              "4  079_o_f_h_a.jpg        079       old  female      happy       a   \n",
              "\n",
              "   face_width  face_height  eye_distance  mouth_width nose_to_mouth  \\\n",
              "0      1859.0       1859.0    682.354746        767.0          None   \n",
              "1      1908.0       1908.0    435.491676        558.0          None   \n",
              "2      1842.0       1842.0           NaN        709.0          None   \n",
              "3      1824.0       1824.0   1025.914226        725.0          None   \n",
              "4      2193.0       2193.0    239.384628        509.0          None   \n",
              "\n",
              "   phi_dev_face_h_w  phi_dev_eye_to_mouth phi_dev_nose_mouth_to_eye  \n",
              "0          0.618034              0.493985                      None  \n",
              "1          0.618034              0.336724                      None  \n",
              "2          0.618034                   NaN                      None  \n",
              "3          0.618034              0.202980                      None  \n",
              "4          0.618034              0.508251                      None  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build dataset from all images in faces_database\n",
        "rows = []\n",
        "image_paths = sorted([p for p in IMAGES_DIR.glob(\"*.jpg\")])\n",
        "\n",
        "for img_path in image_paths:\n",
        "    img = cv2.imread(str(img_path))\n",
        "    if img is None:\n",
        "        continue\n",
        "    labels = parse_labels(img_path.name)\n",
        "    meas = compute_measurements(img)\n",
        "\n",
        "    row = {\n",
        "        # categoricals\n",
        "        \"file_name\": img_path.name,\n",
        "        \"subject_id\": labels.get(\"subject_id\"),\n",
        "        \"age_group\": labels.get(\"age_group\"),\n",
        "        \"gender\": labels.get(\"gender\"),\n",
        "        \"expression\": labels.get(\"expression\"),\n",
        "        \"variant\": labels.get(\"variant\"),\n",
        "        # numericals\n",
        "        **{k: (None if v is None else float(v)) for k, v in meas.items()},\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "faces_df = pd.DataFrame(rows)\n",
        "\n",
        "# Display summary\n",
        "print(f\"Processed {len(rows)} images from: {IMAGES_DIR}\")\n",
        "faces_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file_name                    0.000000\n",
            "subject_id                   0.000000\n",
            "age_group                    0.000000\n",
            "gender                       0.000000\n",
            "expression                   0.000000\n",
            "variant                      0.000000\n",
            "face_width                   0.000000\n",
            "face_height                  0.000000\n",
            "mouth_width                  0.000000\n",
            "phi_dev_face_h_w             0.000000\n",
            "eye_distance                 0.333333\n",
            "phi_dev_eye_to_mouth         0.333333\n",
            "nose_to_mouth                1.000000\n",
            "phi_dev_nose_mouth_to_eye    1.000000\n",
            "dtype: float64\n",
            "Saved: /Users/bellalu/DS6012/ML1_Project/faces_features_golden_ratio.csv\n"
          ]
        }
      ],
      "source": [
        "# Inspect completeness and save results\n",
        "print(faces_df.isna().mean().sort_values())\n",
        "output_csv = PROJECT_ROOT / \"faces_features_golden_ratio.csv\"\n",
        "faces_df.to_csv(output_csv, index=False)\n",
        "print(f\"Saved: {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target column: is_golden_pct\n",
            "Median threshold value: 0.6180339887498949\n",
            "Class balance (value counts):\n",
            "is_golden_pct\n",
            "True    12\n",
            "Name: count, dtype: int64\n",
            "\n",
            "⚠️ WARNING: Only one class in target! Using median split to create balance...\n",
            "Updated class balance:\n",
            "is_golden_pct\n",
            "True    12\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>phi_dev_face_h_w</th>\n",
              "      <th>is_golden_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>004_o_m_h_a.jpg</td>\n",
              "      <td>0.618034</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>004_o_m_h_b.jpg</td>\n",
              "      <td>0.618034</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>066_y_m_h_a.jpg</td>\n",
              "      <td>0.618034</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>066_y_m_h_b.jpg</td>\n",
              "      <td>0.618034</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>079_o_f_h_a.jpg</td>\n",
              "      <td>0.618034</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         file_name  phi_dev_face_h_w  is_golden_pct\n",
              "0  004_o_m_h_a.jpg          0.618034           True\n",
              "1  004_o_m_h_b.jpg          0.618034           True\n",
              "2  066_y_m_h_a.jpg          0.618034           True\n",
              "3  066_y_m_h_b.jpg          0.618034           True\n",
              "4  079_o_f_h_a.jpg          0.618034           True"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define binary targets for golden ratio \"closeness\"\n",
        "import numpy as np\n",
        "\n",
        "# Choose which phi deviation to target primarily\n",
        "PRIMARY_TARGET = \"phi_dev_face_h_w\"  # alternatives: 'phi_dev_eye_to_mouth', 'phi_dev_nose_mouth_to_eye'\n",
        "\n",
        "# Strategy: use median split for balanced classes (50th percentile)\n",
        "# With small datasets, median ensures roughly 50/50 split\n",
        "MEDIAN_SPLIT = True\n",
        "PERCENTILE = 50  # median split for balanced classes\n",
        "\n",
        "# Compute percentile-based target using median (lower deviation is better/closer to golden ratio)\n",
        "valid_devs = faces_df[PRIMARY_TARGET].dropna().values\n",
        "if valid_devs.size > 0:\n",
        "    if MEDIAN_SPLIT:\n",
        "        perc_cut = np.median(valid_devs)  # median split for balance\n",
        "    else:\n",
        "        perc_cut = np.percentile(valid_devs, PERCENTILE)\n",
        "else:\n",
        "    perc_cut = None\n",
        "\n",
        "faces_df[\"is_golden_pct\"] = faces_df[PRIMARY_TARGET].apply(\n",
        "    lambda v: bool(perc_cut is not None and v is not None and v <= perc_cut)\n",
        ")\n",
        "\n",
        "# Alternative: absolute threshold (adjust if needed based on your data range)\n",
        "ABS_THRESHOLD = np.median(valid_devs) if valid_devs.size > 0 else 0.1\n",
        "faces_df[\"is_golden_abs\"] = faces_df[PRIMARY_TARGET].apply(\n",
        "    lambda v: bool(v is not None and v < ABS_THRESHOLD)\n",
        ")\n",
        "\n",
        "# Use median-based target (should be more balanced)\n",
        "TARGET_COL = \"is_golden_pct\"\n",
        "\n",
        "print(\"Target column:\", TARGET_COL)\n",
        "print(\"Median threshold value:\", perc_cut)\n",
        "print(\"Class balance (value counts):\")\n",
        "print(faces_df[TARGET_COL].value_counts(dropna=False))\n",
        "\n",
        "# Check if we have both classes\n",
        "class_counts = faces_df[TARGET_COL].value_counts()\n",
        "if len(class_counts) < 2:\n",
        "    print(\"\\n⚠️ WARNING: Only one class in target! Using median split to create balance...\")\n",
        "    # Force a more balanced split by using quantiles\n",
        "    if valid_devs.size > 0:\n",
        "        median_val = np.median(valid_devs)\n",
        "        faces_df[TARGET_COL] = faces_df[PRIMARY_TARGET].apply(\n",
        "            lambda v: bool(v is not None and v <= median_val)\n",
        "        )\n",
        "        print(\"Updated class balance:\")\n",
        "        print(faces_df[TARGET_COL].value_counts(dropna=False))\n",
        "\n",
        "faces_df[[\"file_name\", PRIMARY_TARGET, TARGET_COL]].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ ERROR: Target has only 1 class(es): [1]\n",
            "Cannot train binary classifier. Please adjust the target threshold in the previous cell.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Need at least 2 classes, found: [1]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ ERROR: Target has only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_classes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m class(es): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot train binary classifier. Please adjust the target threshold in the previous cell.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed at least 2 classes, found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Target has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_classes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdict\u001b[39m(y\u001b[38;5;241m.\u001b[39mvalue_counts())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m numeric_transformer \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     38\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     39\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler()),\n\u001b[1;32m     40\u001b[0m ])\n",
            "\u001b[0;31mValueError\u001b[0m: Need at least 2 classes, found: [1]"
          ]
        }
      ],
      "source": [
        "# Feature preparation: numerical + one-hot categoricals with imputation\n",
        "import sys, subprocess\n",
        "try:\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    from sklearn.pipeline import Pipeline\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"scikit-learn\"])  # noqa: E402\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Select features\n",
        "num_features = [\n",
        "    \"face_width\", \"face_height\", \"eye_distance\", \"mouth_width\", \"nose_to_mouth\",\n",
        "    \"phi_dev_face_h_w\", \"phi_dev_eye_to_mouth\", \"phi_dev_nose_mouth_to_eye\",\n",
        "]\n",
        "cat_features = [\"age_group\", \"gender\", \"expression\", \"variant\"]\n",
        "\n",
        "df_model = faces_df.dropna(subset=[TARGET_COL]).copy()\n",
        "X = df_model[num_features + cat_features]\n",
        "y = df_model[TARGET_COL].astype(int)\n",
        "\n",
        "# Safety check: ensure both classes exist\n",
        "unique_classes = y.unique()\n",
        "if len(unique_classes) < 2:\n",
        "    print(f\"⚠️ ERROR: Target has only {len(unique_classes)} class(es): {unique_classes}\")\n",
        "    print(\"Cannot train binary classifier. Please adjust the target threshold in the previous cell.\")\n",
        "    raise ValueError(f\"Need at least 2 classes, found: {unique_classes}\")\n",
        "\n",
        "print(f\"✓ Target has {len(unique_classes)} classes: {dict(y.value_counts())}\")\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, num_features),\n",
        "        (\"cat\", categorical_transformer, cat_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Use stratify only if both classes have enough samples for stratification\n",
        "min_class_count = y.value_counts().min()\n",
        "stratify_param = y if min_class_count >= 2 else None\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=stratify_param\n",
        ")\n",
        "\n",
        "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n",
        "print(\"Positive rate (train/test):\", y_train.mean().round(3), y_test.mean().round(3))\n",
        "print(f\"Train classes: {dict(y_train.value_counts())}\")\n",
        "print(f\"Test classes: {dict(y_test.value_counts())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.13/site-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['nose_to_mouth' 'phi_dev_nose_mouth_to_eye']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m log_reg_clf \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      7\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocess),\n\u001b[1;32m      8\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m\"\u001b[39m, LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m      9\u001b[0m ])\n\u001b[1;32m     11\u001b[0m rf_clf \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     12\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocess),\n\u001b[1;32m     13\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m\"\u001b[39m, RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     14\u001b[0m ])\n\u001b[0;32m---> 16\u001b[0m log_reg_clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     17\u001b[0m rf_clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels trained.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    661\u001b[0m         )\n\u001b[0;32m--> 662\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1301\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1299\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1305\u001b[0m     )\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1308\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)"
          ]
        }
      ],
      "source": [
        "# Train baseline Logistic Regression and Random Forest\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "log_reg_clf = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\")),\n",
        "])\n",
        "\n",
        "rf_clf = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=300, max_depth=None, random_state=42, class_weight=\"balanced_subsample\")),\n",
        "])\n",
        "\n",
        "log_reg_clf.fit(X_train, y_train)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Models trained.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate models: metrics + cross-validation\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "models = {\n",
        "    \"log_reg\": log_reg_clf,\n",
        "    \"random_forest\": rf_clf,\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_test, y_prob)\n",
        "    except ValueError:\n",
        "        auc = float('nan')\n",
        "\n",
        "    print(f\"\\n=== {name} (test) ===\")\n",
        "    print(f\"Accuracy: {acc:.3f}  F1: {f1:.3f}  Precision: {prec:.3f}  Recall: {rec:.3f}  ROC AUC: {auc:.3f}\")\n",
        "    print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "    # cross-validated ROC AUC on training set (only if both classes present in train)\n",
        "    train_class_counts = y_train.value_counts()\n",
        "    if len(train_class_counts) >= 2 and train_class_counts.min() >= 1:\n",
        "        n_splits = min(5, max(2, train_class_counts.min()))\n",
        "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "        try:\n",
        "            cv_auc = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"roc_auc\")\n",
        "            print(f\"CV ROC AUC (mean±std): {cv_auc.mean():.3f} ± {cv_auc.std():.3f}\")\n",
        "        except ValueError as e:\n",
        "            print(f\"CV skipped: {e}\")\n",
        "    else:\n",
        "        print(f\"CV skipped: insufficient class balance in training set ({dict(train_class_counts)})\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
